{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chenoa23/CV-Projects/blob/main/Building_Blocks_of_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0305546f-f1de-4b67-a203-bdbfe4a98ab0",
      "metadata": {
        "id": "0305546f-f1de-4b67-a203-bdbfe4a98ab0"
      },
      "source": [
        "# Convolutions and Pooling operations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1db92d5-7f67-473b-8757-6c9f1ca5cb81",
      "metadata": {
        "tags": [],
        "id": "b1db92d5-7f67-473b-8757-6c9f1ca5cb81"
      },
      "source": [
        "### Load the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bo_E7vBvHLHh",
        "outputId": "46841b65-89b2-47c4-d67f-1dbddb7fe8cd"
      },
      "id": "Bo_E7vBvHLHh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6dcb6d1-6bda-40fd-abb9-829e5f8b1967",
      "metadata": {
        "id": "c6dcb6d1-6bda-40fd-abb9-829e5f8b1967"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31b2e90a-b771-412c-8dea-1bb31ff2e167",
      "metadata": {
        "id": "31b2e90a-b771-412c-8dea-1bb31ff2e167"
      },
      "source": [
        "### Let's define some filters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "432ae5ef-3e7b-4c45-b41e-5aaf490622b6",
      "metadata": {
        "id": "432ae5ef-3e7b-4c45-b41e-5aaf490622b6"
      },
      "outputs": [],
      "source": [
        "# naive filter\n",
        "filter_naive = \\\n",
        "           [[+0.0, +0.0, +0.0],\n",
        "            [+0.0, +1.0, +0.0],\n",
        "            [+0.0, +0.0, +0.0]]\n",
        "\n",
        "# filter for detecting vertical lines\n",
        "filter_vertical = \\\n",
        "           [[+1.0, +0.0, -1.0],\n",
        "            [+1.0, +0.0, -1.0],\n",
        "            [+1.0, +0.0, -1.0]]\n",
        "\n",
        "# filter for detecting horizontal lines\n",
        "filter_horizontal = \\\n",
        "           [[+1.0, +1.0, +1.0],\n",
        "            [+0.0, +0.0, +0.0],\n",
        "            [-1.0, -1.0, -1.0]]\n",
        "\n",
        "\n",
        "# filter for detecting obliquous lines\n",
        "filter_obliquous = \\\n",
        "           [[-1.0, -1.0, +2.0],\n",
        "            [-1.0, +2.0, -1.0],\n",
        "            [+2.0, -1.0, -1.0]]\n",
        "\n",
        "# filter for getting blurry images\n",
        "filter_blur = \\\n",
        "           [[+1.0, +1.0, +1.0],\n",
        "            [+1.0, +1.0, +1.0],\n",
        "            [+1.0, +1.0, +1.0]]\n",
        "\n",
        "# filter for getting sharper edges\n",
        "filter_edges = \\\n",
        "           [[+1.0, +1.0, +1.0],\n",
        "            [+1.0, -7.0, +1.0],\n",
        "            [+1.0, +1.0, +1.0]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e82fdb03-d0a8-46cf-8d27-5e6ec5ec36e2",
      "metadata": {
        "tags": [],
        "id": "e82fdb03-d0a8-46cf-8d27-5e6ec5ec36e2"
      },
      "source": [
        "## Convolution Operation\n",
        "The first key concept that we need to understand is the convolution operation. This is simple: we will apply a filter to an image to get a resulting image.\n",
        "\n",
        "<img src=\"./imgs/conv.png\" width=\"800\" height=\"400\" align=\"center\">\n",
        "\n",
        "Image retrieved from <a href=\"https://medium.datadriveninvestor.com/convolutional-neural-networks-3b241a5da51e\">Link</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9463188-3e9d-4260-9826-98e44e63b5b4",
      "metadata": {
        "id": "e9463188-3e9d-4260-9826-98e44e63b5b4"
      },
      "source": [
        "Function to apply filter and display the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12b50479-356b-4169-83a4-226fdb290d88",
      "metadata": {
        "id": "12b50479-356b-4169-83a4-226fdb290d88"
      },
      "outputs": [],
      "source": [
        "def apply_filter(image, filter):\n",
        "\n",
        "  img = load_img(image,color_mode='grayscale')\n",
        "  img_array = img_to_array(img)\n",
        "  i = img_array.flatten().reshape(img_array.shape[0], img_array.shape[1])\n",
        "\n",
        "  i_transformed = np.copy(i)\n",
        "  size_x = i_transformed.shape[0]\n",
        "  size_y = i_transformed.shape[1]\n",
        "\n",
        "  if np.sum(filter) == 0:\n",
        "    weight = 1.\n",
        "  else:\n",
        "    weight = 1 / np.sum(filter)\n",
        "\n",
        "  for x in range(1,size_x-1):\n",
        "    for y in range(1,size_y-1):\n",
        "        convolution = 0.0\n",
        "        convolution = convolution + (i[x-1, y-1] * filter[0][0])\n",
        "        convolution = convolution + (i[x-1, y] * filter[0][1])\n",
        "        convolution = convolution + (i[x-1, y+1] * filter[0][2])\n",
        "        convolution = convolution + (i[x, y-1] * filter[1][0])\n",
        "        convolution = convolution + (i[x, y] * filter[1][1])\n",
        "        convolution = convolution + (i[x, y+1] * filter[1][2])\n",
        "        convolution = convolution + (i[x+1, y-1] * filter[2][0])\n",
        "        convolution = convolution + (i[x+1, y] * filter[2][1])\n",
        "        convolution = convolution + (i[x+1, y+1] * filter[2][2])\n",
        "        convolution = convolution * weight\n",
        "        if(convolution<0):\n",
        "          convolution=0\n",
        "        if(convolution>255):\n",
        "          convolution=255\n",
        "        i_transformed[x, y] = convolution\n",
        "\n",
        "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24,8))\n",
        "  fig.suptitle('Convolution operation')\n",
        "  plt.gray()\n",
        "  plt.grid(False)\n",
        "  ax1.imshow(img)\n",
        "  ax1.set_title(\"Original Image\")\n",
        "  ax2.imshow(i_transformed)\n",
        "  ax2.set_title(\"Image after convolution\")\n",
        "  plt.axis('on')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4a87fd7-8828-468c-b14d-d661383a0b8e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "a4a87fd7-8828-468c-b14d-d661383a0b8e",
        "outputId": "de43a9a5-718e-4ecd-9e73-d805b3cf24e9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'filtername' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-b28d5cce3d82>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mapply_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/FELV-cat-1558357177.png\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfiltername\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# example:  apply_filter('./Images/1.jpg', filter_edges)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Sample images can be found in Images folder.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'filtername' is not defined"
          ]
        }
      ],
      "source": [
        "apply_filter(\"/content/FELV-cat-1558357177.\",filtername)\n",
        "\n",
        "# example:  apply_filter('./Images/1.jpg', filter_edges)\n",
        "\n",
        "# Sample images can be found in Images folder.\n",
        "\n",
        "# Different filters have been defined above, there are 5 of them defined, please use the same function\n",
        "# for both convolution and pooling to understand the impacts of each of the operation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf68d578-a841-4724-9cd9-79f425a859fc",
      "metadata": {
        "id": "bf68d578-a841-4724-9cd9-79f425a859fc"
      },
      "source": [
        "</p> As expected, only the features that are required for us have been extracted from the image after convolution. (Ex, when we apply the \"filter_edges\", we get the edges extracted from the image) </p>\n",
        "\n",
        "<hr size=\"5\" width=\"100%\" color=\"red\">  \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0c298d2-bf97-41bb-ad00-c903d0a20384",
      "metadata": {
        "tags": [],
        "id": "a0c298d2-bf97-41bb-ad00-c903d0a20384"
      },
      "source": [
        "## Pooling operation\n",
        "\n",
        "\n",
        "<img src=\"./imgs/pooling.png\" width=\"800\" height=\"400\">\n",
        "\n",
        "Image retrieved from : <a href=\"https://towardsdatascience.com/understanding-convolutions-and-pooling-in-neural-networks-a-simple-explanation-885a2d78f211\">Link</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93c0d26a-a787-4bc6-8054-b6130922fa1b",
      "metadata": {
        "id": "93c0d26a-a787-4bc6-8054-b6130922fa1b"
      },
      "outputs": [],
      "source": [
        "def apply_filter_pool(image, filter):\n",
        "\n",
        "  img_pre = load_img(image,color_mode='grayscale')\n",
        "  x_img_pre = img_pre.size[0]\n",
        "  y_img_pre = img_pre.size[1]\n",
        "  img = load_img(image,color_mode='grayscale', target_size=(int(np.ceil(y_img_pre)//2*2),int(np.ceil(x_img_pre)//2*2)))\n",
        "  img_array = img_to_array(img)\n",
        "  i = img_array.flatten().reshape(img_array.shape[0], img_array.shape[1])\n",
        "\n",
        "  i_transformed = np.copy(i)\n",
        "  size_x = i_transformed.shape[0]\n",
        "  size_y = i_transformed.shape[1]\n",
        "\n",
        "  if np.sum(filter) == 0:\n",
        "    weight = 1.\n",
        "  else:\n",
        "    weight = 1 / np.sum(filter)\n",
        "\n",
        "  for x in range(1,size_x-1):\n",
        "    for y in range(1,size_y-1):\n",
        "        convolution = 0.0\n",
        "        convolution = convolution + (i[x-1, y-1] * filter[0][0])\n",
        "        convolution = convolution + (i[x-1, y] * filter[0][1])\n",
        "        convolution = convolution + (i[x-1, y+1] * filter[0][2])\n",
        "        convolution = convolution + (i[x, y-1] * filter[1][0])\n",
        "        convolution = convolution + (i[x, y] * filter[1][1])\n",
        "        convolution = convolution + (i[x, y+1] * filter[1][2])\n",
        "        convolution = convolution + (i[x+1, y-1] * filter[2][0])\n",
        "        convolution = convolution + (i[x+1, y] * filter[2][1])\n",
        "        convolution = convolution + (i[x+1, y+1] * filter[2][2])\n",
        "        convolution = convolution * weight\n",
        "        if(convolution<0):\n",
        "          convolution=0\n",
        "        if(convolution>255):\n",
        "          convolution=255\n",
        "        i_transformed[x, y] = convolution\n",
        "\n",
        "  new_x = int(size_x/2)\n",
        "  new_y = int(size_y/2)\n",
        "  newImage = np.zeros((new_x, new_y))\n",
        "  for x in range(0, size_x, 2):\n",
        "    for y in range(0, size_y, 2):\n",
        "      pixels = []\n",
        "      pixels.append(i_transformed[x, y])\n",
        "      pixels.append(i_transformed[x+1, y])\n",
        "      pixels.append(i_transformed[x, y+1])\n",
        "      pixels.append(i_transformed[x+1, y+1])\n",
        "      newImage[int(x/2),int(y/2)] = max(pixels)  # try with average\n",
        "  a = \" \"\n",
        "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24,8))\n",
        "  fig.suptitle('Pooling operation')\n",
        "  plt.gray()\n",
        "  plt.grid(False)\n",
        "  ax2.imshow(newImage)\n",
        "  ax2.set_title(\"Image after pooling: Shape - %s\" %(newImage.shape,))\n",
        "  ax1.imshow(i_transformed)\n",
        "  ax1.set_title(\"Image after convolution: Shape - %s\" %(i_transformed.shape,))\n",
        "  plt.axis('on')\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7246f40e-97ff-423f-8804-95aaab7dc29d",
      "metadata": {
        "id": "7246f40e-97ff-423f-8804-95aaab7dc29d"
      },
      "outputs": [],
      "source": [
        "apply_filter_pool(\"Path-to-image\",filtername)\n",
        "\n",
        "# example: apply_filter_pool('./Images/1.jpg', filter_obliquous)\n",
        "\n",
        "# Sample images can be found in Images folder.\n",
        "\n",
        "# Different filters have been defined above, there are 5 of them defined, please use the same function\n",
        "# for both convolution and pooling to understand the impacts of each of the operation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be6daaa4-26ec-40aa-8719-15fb5a4fd305",
      "metadata": {
        "id": "be6daaa4-26ec-40aa-8719-15fb5a4fd305"
      },
      "source": [
        "Things to observe:\n",
        "1. The image size has been reduced by half because of taking 2x2 pooling. Observe the shape of images before and after pooling\n",
        "2. The information we demanded through convolution layers has been intensified and much enhanced"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}