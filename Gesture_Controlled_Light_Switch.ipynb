{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chenoa23/CV-Projects/blob/main/Gesture_Controlled_Light_Switch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBIf0-TB1f15"
      },
      "source": [
        "Ignacio Calvo\\\n",
        "Chenoa Nussberger\\\n",
        "Paula Andrea Gallego\\\n",
        "Sheena Johns\n",
        "\n",
        "\n",
        "# **Gesture-Controlled Light Switch Using Computer Vision**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Project Purpose / Point\n",
        "This project implements a gesture-controlled light switch using computer vision and deep learning. It uses a labeled dataset of hand gesture images to train a convolutional neural network (CNN) based on MobileNetV2 architecture. The trained model can recognize multiple hand gestures in real-time video input. By detecting specific hand gestures, the system toggles the state of a virtual light switch (on/off). The project integrates MediaPipe for hand landmark detection and TensorFlow for gesture classification, demonstrating practical application of computer vision for intuitive, contactless control interfaces.\n",
        "\n"
      ],
      "metadata": {
        "id": "r93l4VTEn9Od"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzalqN_q-luQ"
      },
      "source": [
        "# Dataset Inspection and Structure Overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhogC2cIyqtG",
        "outputId": "7d5caf84-98b3-4b3c-b453-1051eeaa5b80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Directory Structure:\n",
            "Class '6' has 900 images\n",
            "Class '18' has 900 images\n",
            "Class '9' has 940 images\n",
            "Class '0' has 900 images\n",
            "Class '10' has 900 images\n",
            "Class '1' has 900 images\n",
            "Class '11' has 900 images\n",
            "Class '7' has 900 images\n",
            "Class '17' has 900 images\n",
            "Class '16' has 900 images\n",
            "Class '19' has 900 images\n",
            "Class '14' has 900 images\n",
            "Class '13' has 900 images\n",
            "Class '15' has 900 images\n",
            "Class '12' has 900 images\n",
            "Class '4' has 930 images\n",
            "Class '5' has 900 images\n",
            "Class '3' has 900 images\n",
            "Class '2' has 900 images\n",
            "Class '8' has 900 images\n",
            "\n",
            "Test Directory Structure:\n",
            "Class '11' has 300 images\n",
            "Class '6' has 320 images\n",
            "Class '1' has 300 images\n",
            "Class '18' has 300 images\n",
            "Class '17' has 310 images\n",
            "Class '16' has 300 images\n",
            "Class '7' has 300 images\n",
            "Class '0' has 300 images\n",
            "Class '10' has 300 images\n",
            "Class '9' has 300 images\n",
            "Class '13' has 300 images\n",
            "Class '8' has 300 images\n",
            "Class '12' has 300 images\n",
            "Class '5' has 300 images\n",
            "Class '3' has 300 images\n",
            "Class '4' has 300 images\n",
            "Class '19' has 300 images\n",
            "Class '2' has 300 images\n",
            "Class '15' has 300 images\n",
            "Class '14' has 300 images\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Define dataset paths\n",
        "train_dir = '/content/drive/MyDrive/Fall 2024/CAI 2840C Computer Vision/archive 1/train/train'\n",
        "test_dir = '/content/drive/MyDrive/Fall 2024/CAI 2840C Computer Vision/archive 1/test/test'\n",
        "\n",
        "def count_images_in_subfolders(base_dir):\n",
        "    if not os.path.isdir(base_dir):\n",
        "        print(f\"Error: Directory not found at {base_dir}\")\n",
        "        return\n",
        "\n",
        "    for class_folder in os.listdir(base_dir):\n",
        "        class_folder_path = os.path.join(base_dir, class_folder)\n",
        "        if os.path.isdir(class_folder_path):  # Check if it's a directory\n",
        "            num_images = len([\n",
        "                file for file in os.listdir(class_folder_path)\n",
        "                if os.path.isfile(os.path.join(class_folder_path, file))\n",
        "            ])\n",
        "            print(f\"Class '{class_folder}' has {num_images} images\")\n",
        "        else:\n",
        "            print(f\"Unexpected file in directory: {class_folder}\")\n",
        "\n",
        "# Check contents of the train directory\n",
        "print(\"Train Directory Structure:\")\n",
        "count_images_in_subfolders(train_dir)\n",
        "\n",
        "# Check contents of the test directory\n",
        "print(\"\\nTest Directory Structure:\")\n",
        "count_images_in_subfolders(test_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-jehO_SdUDB"
      },
      "source": [
        "# Model Preparation, Training, and Saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tq2nOcWY1B1s",
        "outputId": "a8b635c6-3576-421a-ba76-0d3c67878705"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.11/dist-packages (0.10.21)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.2.10)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.2)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (3.10.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.11.0.86)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.25.8)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (1.15.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install mediapipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQ-w2Z6IfEnj",
        "outputId": "4140a28f-2d51-402c-a472-929006862847"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 18070 images belonging to 20 classes.\n",
            "Found 6030 images belonging to 1 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_128_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m  83/1129\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:07:04\u001b[0m 7s/step - accuracy: 0.4189 - loss: 2.0348"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Set up directories for training and testing data\n",
        "train_dir = '/content/drive/MyDrive/Fall 2024/CAI 2840C Computer Vision/archive 1/train/train'\n",
        "test_dir = '/content/drive/MyDrive/Fall 2024/CAI 2840C Computer Vision/archive 1/test'\n",
        "\n",
        "# Data Augmentation for the training set\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255,            # Normalize pixel values to [0, 1]\n",
        "    rotation_range=20,          # Randomly rotate images\n",
        "    width_shift_range=0.2,      # Randomly shift images horizontally\n",
        "    height_shift_range=0.2,     # Randomly shift images vertically\n",
        "    shear_range=0.2,            # Shear transformations\n",
        "    zoom_range=0.2,             # Random zoom\n",
        "    horizontal_flip=True,       # Flip images horizontally\n",
        "    fill_mode='nearest'         # Fill missing pixels after transformations\n",
        ")\n",
        "\n",
        "# No augmentation for validation/test\n",
        "test_datagen = ImageDataGenerator(rescale=1.0/255)\n",
        "\n",
        "# Create the ImageDataGenerators for training and testing\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(128, 128),     # Resize images to 128x128\n",
        "    batch_size=16,              # Use a smaller batch size\n",
        "    class_mode='categorical',   # Multi-class classification\n",
        "    shuffle=True                # Shuffle data for training\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(128, 128),     # Resize images to 128x128\n",
        "    batch_size=8,               # Smaller batch for testing\n",
        "    class_mode='categorical'    # Multi-class classification\n",
        ")\n",
        "\n",
        "# Use Transfer Learning with MobileNetV2\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "\n",
        "# Add custom layers on top\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
        "\n",
        "# Combine the base model and custom layers\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze the base model layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Set up early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Set up checkpoint to save the best model\n",
        "checkpoint = ModelCheckpoint('gesture_model.keras', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    epochs=10,  # Start with fewer epochs\n",
        "    validation_data=test_generator,\n",
        "    validation_steps=test_generator.samples // test_generator.batch_size,\n",
        "    callbacks=[checkpoint, early_stopping]\n",
        ")\n",
        "\n",
        "# Save the final model\n",
        "model.save('gesture_model.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRRh8ueo_VAv"
      },
      "source": [
        "#  Gesture Recognition and Light Control with MediaPipe and TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPYBeFPEWpuC"
      },
      "outputs": [],
      "source": [
        "# Load the trained model\n",
        "model = load_model('gesture_model.keras')\n",
        "\n",
        "# Class labels\n",
        "class_labels = ['Gesture 0', 'Gesture 1', 'Gesture 2', 'Gesture 3', 'Gesture 4',\n",
        "                'Gesture 5', 'Gesture 6', 'Gesture 7', 'Gesture 8', 'Gesture 9',\n",
        "                'Gesture 10', 'Gesture 11', 'Gesture 12', 'Gesture 13', 'Gesture 14',\n",
        "                'Gesture 15', 'Gesture 16', 'Gesture 17', 'Gesture 18', 'Gesture 19']\n",
        "\n",
        "# MediaPipe setup\n",
        "mp_hands = mp.solutions.hands\n",
        "mp_draw = mp.solutions.drawing_utils\n",
        "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
        "\n",
        "# Load video\n",
        "video_path = '/content/drive/MyDrive/Fall 2024/CAI 2840C Computer Vision/Movie on 12-3-24 at 12.00 PM.mov'\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "if not cap.isOpened():\n",
        "    print(\"Error: Unable to open video file.\")\n",
        "else:\n",
        "    print(\"Video loaded successfully.\")\n",
        "\n",
        "# Light state\n",
        "light_on = False\n",
        "\n",
        "# Main loop for video processing\n",
        "while cap.isOpened():\n",
        "    success, frame = cap.read()\n",
        "    if not success:\n",
        "        print(\"End of video reached.\")\n",
        "        break\n",
        "\n",
        "    # Flip and process the frame\n",
        "    frame = cv2.flip(frame, 1)\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    results = hands.process(frame_rgb)\n",
        "\n",
        "    roi = None\n",
        "    if results.multi_hand_landmarks:\n",
        "        for hand_landmarks in results.multi_hand_landmarks:\n",
        "            mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
        "\n",
        "            # Get bounding box around the detected hand\n",
        "            h, w, _ = frame.shape\n",
        "            bbox_x_min = int(min([lm.x for lm in hand_landmarks.landmark]) * w)\n",
        "            bbox_y_min = int(min([lm.y for lm in hand_landmarks.landmark]) * h)\n",
        "            bbox_x_max = int(max([lm.x for lm in hand_landmarks.landmark]) * w)\n",
        "            bbox_y_max = int(max([lm.y for lm in hand_landmarks.landmark]) * h)\n",
        "\n",
        "            # Define ROI dynamically based on bounding box\n",
        "            roi = frame[bbox_y_min:bbox_y_max, bbox_x_min:bbox_x_max]\n",
        "            cv2.rectangle(frame, (bbox_x_min, bbox_y_min), (bbox_x_max, bbox_y_max), (255, 0, 0), 2)\n",
        "\n",
        "    # Predict the gesture if ROI is valid\n",
        "    if roi is not None and roi.size > 0:\n",
        "        roi_resized = cv2.resize(roi, (128, 128)) / 255.0  # Normalize\n",
        "        roi_resized = np.expand_dims(roi_resized, axis=0)  # Add batch dimension\n",
        "\n",
        "        # Predict the gesture\n",
        "        predictions = model.predict(roi_resized)\n",
        "        predicted_class = np.argmax(predictions)\n",
        "        confidence = np.max(predictions)\n",
        "\n",
        "        # Toggle light state if the confidence is high\n",
        "        if confidence > 0.7:  # Threshold to avoid false triggers\n",
        "            print(f\"Predicted Gesture: {class_labels[predicted_class]} (Confidence: {confidence:.2f})\")\n",
        "            light_on = not light_on\n",
        "\n",
        "    # Display light state\n",
        "    status = \"Light ON\" if light_on else \"Light OFF\"\n",
        "    color = (0, 255, 0) if light_on else (0, 0, 255)\n",
        "    cv2.putText(frame, status, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
        "\n",
        "    # Display the frame in Colab\n",
        "    cv2_imshow(frame)\n",
        "\n",
        "# Release resources\n",
        "cap.release()\n",
        "print(\"Video processing completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOQlsp9XZnoE"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1y5p-KbmqwVWYmlSR69UYhgWFY8gKLiSj",
      "authorship_tag": "ABX9TyPiypcjrlDo2LhtHF2gDUvc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}