{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chenoa23/CV-Projects/blob/main/Gesture_Controlled_Light_Switch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBIf0-TB1f15"
      },
      "source": [
        "Ignacio Calvo\\\n",
        "Chenoa Nussberger\\\n",
        "Paula Andrea Gallego\\\n",
        "Sheena Johns\n",
        "\n",
        "\n",
        "# **Gesture-Controlled Light Switch Using Computer Vision**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Project Purpose / Point\n",
        "This project implements a gesture-controlled light switch using computer vision and deep learning. It uses a labeled dataset of hand gesture images to train a convolutional neural network (CNN) based on MobileNetV2 architecture. The trained model can recognize multiple hand gestures in real-time video input. By detecting specific hand gestures, the system toggles the state of a virtual light switch (on/off). The project integrates MediaPipe for hand landmark detection and TensorFlow for gesture classification, demonstrating practical application of computer vision for intuitive, contactless control interfaces.\n",
        "\n"
      ],
      "metadata": {
        "id": "r93l4VTEn9Od"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzalqN_q-luQ"
      },
      "source": [
        "# Dataset Inspection and Structure Overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "OhogC2cIyqtG",
        "outputId": "ca20cd2b-f2ee-47d5-b7e2-29906ea97af6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Directory Structure:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/CAI 2840C/archive 1/train/train'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-2221017444.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Check contents of the train directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train Directory Structure:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mcount_images_in_subfolders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Check contents of the test directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1-2221017444.py\u001b[0m in \u001b[0;36mcount_images_in_subfolders\u001b[0;34m(base_dir)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcount_images_in_subfolders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mclass_folder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mclass_folder_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_folder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Check if it's a directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/CAI 2840C/archive 1/train/train'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Define dataset paths\n",
        "train_dir = '/content/drive/MyDrive/CAI 2840C/archive 1/train/train'\n",
        "test_dir = '/content/drive/MyDrive/CAI 2840C/archive 1/test/test'\n",
        "\n",
        "def count_images_in_subfolders(base_dir):\n",
        "    for class_folder in os.listdir(base_dir):\n",
        "        class_folder_path = os.path.join(base_dir, class_folder)\n",
        "        if os.path.isdir(class_folder_path):  # Check if it's a directory\n",
        "            num_images = len([\n",
        "                file for file in os.listdir(class_folder_path)\n",
        "                if os.path.isfile(os.path.join(class_folder_path, file))\n",
        "            ])\n",
        "            print(f\"Class '{class_folder}' has {num_images} images\")\n",
        "        else:\n",
        "            print(f\"Unexpected file in directory: {class_folder}\")\n",
        "\n",
        "# Check contents of the train directory\n",
        "print(\"Train Directory Structure:\")\n",
        "count_images_in_subfolders(train_dir)\n",
        "\n",
        "# Check contents of the test directory\n",
        "print(\"\\nTest Directory Structure:\")\n",
        "count_images_in_subfolders(test_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-jehO_SdUDB"
      },
      "source": [
        "# Model Preparation, Training, and Saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tq2nOcWY1B1s"
      },
      "outputs": [],
      "source": [
        "!pip install mediapipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQ-w2Z6IfEnj"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Set up directories for training and testing data\n",
        "train_dir = '/content/drive/MyDrive/CAI 2840C/archive 1/train/train'\n",
        "test_dir = '/content/drive/MyDrive/CAI 2840C/archive 1/test/test'\n",
        "\n",
        "# Data Augmentation for the training set\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255,            # Normalize pixel values to [0, 1]\n",
        "    rotation_range=20,          # Randomly rotate images\n",
        "    width_shift_range=0.2,      # Randomly shift images horizontally\n",
        "    height_shift_range=0.2,     # Randomly shift images vertically\n",
        "    shear_range=0.2,            # Shear transformations\n",
        "    zoom_range=0.2,             # Random zoom\n",
        "    horizontal_flip=True,       # Flip images horizontally\n",
        "    fill_mode='nearest'         # Fill missing pixels after transformations\n",
        ")\n",
        "\n",
        "# No augmentation for validation/test\n",
        "test_datagen = ImageDataGenerator(rescale=1.0/255)\n",
        "\n",
        "# Create the ImageDataGenerators for training and testing\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(128, 128),     # Resize images to 128x128\n",
        "    batch_size=16,              # Use a smaller batch size\n",
        "    class_mode='categorical',   # Multi-class classification\n",
        "    shuffle=True                # Shuffle data for training\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(128, 128),     # Resize images to 128x128\n",
        "    batch_size=8,               # Smaller batch for testing\n",
        "    class_mode='categorical'    # Multi-class classification\n",
        ")\n",
        "\n",
        "# Use Transfer Learning with MobileNetV2\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "\n",
        "# Add custom layers on top\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
        "\n",
        "# Combine the base model and custom layers\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze the base model layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Set up early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Set up checkpoint to save the best model\n",
        "checkpoint = ModelCheckpoint('gesture_model.keras', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    epochs=10,  # Start with fewer epochs\n",
        "    validation_data=test_generator,\n",
        "    validation_steps=test_generator.samples // test_generator.batch_size,\n",
        "    callbacks=[checkpoint, early_stopping]\n",
        ")\n",
        "\n",
        "# Save the final model\n",
        "model.save('gesture_model.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRRh8ueo_VAv"
      },
      "source": [
        "#  Gesture Recognition and Light Control with MediaPipe and TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPYBeFPEWpuC"
      },
      "outputs": [],
      "source": [
        "# Load the trained model\n",
        "model = load_model('gesture_model.keras')\n",
        "\n",
        "# Class labels\n",
        "class_labels = ['Gesture 0', 'Gesture 1', 'Gesture 2', 'Gesture 3', 'Gesture 4',\n",
        "                'Gesture 5', 'Gesture 6', 'Gesture 7', 'Gesture 8', 'Gesture 9',\n",
        "                'Gesture 10', 'Gesture 11', 'Gesture 12', 'Gesture 13', 'Gesture 14',\n",
        "                'Gesture 15', 'Gesture 16', 'Gesture 17', 'Gesture 18', 'Gesture 19']\n",
        "\n",
        "# MediaPipe setup\n",
        "mp_hands = mp.solutions.hands\n",
        "mp_draw = mp.solutions.drawing_utils\n",
        "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
        "\n",
        "# Load video\n",
        "video_path = '/content/drive/MyDrive/Movie on 12-3-24 at 12.00 PM.mov'\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "if not cap.isOpened():\n",
        "    print(\"Error: Unable to open video file.\")\n",
        "else:\n",
        "    print(\"Video loaded successfully.\")\n",
        "\n",
        "# Light state\n",
        "light_on = False\n",
        "\n",
        "# Main loop for video processing\n",
        "while cap.isOpened():\n",
        "    success, frame = cap.read()\n",
        "    if not success:\n",
        "        print(\"End of video reached.\")\n",
        "        break\n",
        "\n",
        "    # Flip and process the frame\n",
        "    frame = cv2.flip(frame, 1)\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    results = hands.process(frame_rgb)\n",
        "\n",
        "    roi = None\n",
        "    if results.multi_hand_landmarks:\n",
        "        for hand_landmarks in results.multi_hand_landmarks:\n",
        "            mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
        "\n",
        "            # Get bounding box around the detected hand\n",
        "            h, w, _ = frame.shape\n",
        "            bbox_x_min = int(min([lm.x for lm in hand_landmarks.landmark]) * w)\n",
        "            bbox_y_min = int(min([lm.y for lm in hand_landmarks.landmark]) * h)\n",
        "            bbox_x_max = int(max([lm.x for lm in hand_landmarks.landmark]) * w)\n",
        "            bbox_y_max = int(max([lm.y for lm in hand_landmarks.landmark]) * h)\n",
        "\n",
        "            # Define ROI dynamically based on bounding box\n",
        "            roi = frame[bbox_y_min:bbox_y_max, bbox_x_min:bbox_x_max]\n",
        "            cv2.rectangle(frame, (bbox_x_min, bbox_y_min), (bbox_x_max, bbox_y_max), (255, 0, 0), 2)\n",
        "\n",
        "    # Predict the gesture if ROI is valid\n",
        "    if roi is not None and roi.size > 0:\n",
        "        roi_resized = cv2.resize(roi, (128, 128)) / 255.0  # Normalize\n",
        "        roi_resized = np.expand_dims(roi_resized, axis=0)  # Add batch dimension\n",
        "\n",
        "        # Predict the gesture\n",
        "        predictions = model.predict(roi_resized)\n",
        "        predicted_class = np.argmax(predictions)\n",
        "        confidence = np.max(predictions)\n",
        "\n",
        "        # Toggle light state if the confidence is high\n",
        "        if confidence > 0.7:  # Threshold to avoid false triggers\n",
        "            print(f\"Predicted Gesture: {class_labels[predicted_class]} (Confidence: {confidence:.2f})\")\n",
        "            light_on = not light_on\n",
        "\n",
        "    # Display light state\n",
        "    status = \"Light ON\" if light_on else \"Light OFF\"\n",
        "    color = (0, 255, 0) if light_on else (0, 0, 255)\n",
        "    cv2.putText(frame, status, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
        "\n",
        "    # Display the frame in Colab\n",
        "    cv2_imshow(frame)\n",
        "\n",
        "# Release resources\n",
        "cap.release()\n",
        "print(\"Video processing completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOQlsp9XZnoE"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1y5p-KbmqwVWYmlSR69UYhgWFY8gKLiSj",
      "authorship_tag": "ABX9TyPMjT0RzWML6WpeaXbrs7DD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}